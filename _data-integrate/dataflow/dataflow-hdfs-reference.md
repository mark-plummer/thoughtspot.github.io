---
title: [HDFS connection reference]
summary: Learn about the fields used to create an HDFS connection with ThoughtSpot DataFlow.
last_updated: 07/07/2020
redirect_from:
- /7.0.0.mar.sw/data-integrate/dataflow/dataflow-hdfs-reference.html
- /7.0.1.jun.sw/data-integrate/dataflow/dataflow-hdfs-reference.html
sidebar: mydoc_sidebar
permalink: /:collection/:path.html
---

Here is a list of the fields for an HDFS connection in ThoughtSpot DataFlow. You need specific information to establish a seamless and secure connection.

## Connection properties

<dl id="dataflow-hdfs-connection-properties">
<dlentry id="dataflow-hdfs-conn-connection-name"><dt>Connection name</dt><dd id="connection-name-description">Name your connection.</dd><dd id="connection-name-required">Mandatory field.</dd><dd id="connection-name-example"><strong>Example:</strong><br/>HDFSConnection</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-connection-type"><dt>Connection type</dt><dd id="connection-type-description">Choose the Google BigQuery connection type.</dd><dd id="connection-type-required">Mandatory field.</dd><dd id="connection-type-example"><strong>Example:</strong><br/>HDFS</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-user"><dt>User</dt><dd id="user-description">Specify the user to connect to HDFS file system. This user must have data access privileges.</dd><dd id="user-required">Mandatory field.<br/>For Hive security with simple, LDAP, and SSL authentication only.</dd><dd id="user-example"><strong>Example:</strong><br/>user1</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-hadoop-distribution"><dt>Hadoop distribution</dt><dd id="hadoop-distribution-description">Provide the distribution of Hadoop being connected to</dd><dd id="hadoop-distribution-required">Mandatory field.</dd><dd id="hadoop-distribution-example"><strong>Example:</strong><br/>Hortonworks</dd><dd id="hadoop-distribution-valid-values"><strong>Valid Values:</strong><br/>CDH, Hortonworks, EMR</dd><dd id="hadoop-distribution-default"><strong>Default:</strong><br/>CDH</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-distribution-version"><dt>Distribution version</dt><dd id="distribution-version-description">Provide the version of the Distribution chosen above</dd><dd id="distribution-version-required">Mandatory field.</dd><dd id="distribution-version-example"><strong>Example:</strong><br/>2.6.5</dd><dd id="distribution-version-valid-values"><strong>Valid Values:</strong><br/>Valid distribution number of the <strong>Hadoop distribution</strong></dd><dd id="distribution-version-default"><strong>Default:</strong><br/>6.3.x</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-hadoop-conf-path"><dt>Hadoop conf path</dt><dd id="hadoop-conf-path-description">By default, the system picks the Hadoop configuration files from the HDFS. To override, specify an alternate location. Applies only when using configuration settings that are different from global Hadoop instance settings.</dd><dd id="hadoop-conf-path-required">Mandatory field.</dd><dd id="hadoop-conf-path-example"><strong>Example:</strong><br/>/app/path</dd><dd id="hadoop-conf-path-other"><strong>Other notes:</strong><br/>An instance where this could be needed is, if the hdfs is encrypted and the location of key files and password decrypt the files is available in the hadoop config files.</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-hdfs-ha-configured"><dt>HDFS HA configured</dt><dd id="hdfs-ha-configured-description">Enables High Availability for HDFS</dd><dd id="hdfs-ha-configured-required">Optional field.</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-hdfs-name-service"><dt>HDFS name service</dt><dd id="hdfs-name-service-description">The logical name of given to HDFS nameservice. </dd><dd id="hdfs-name-service-required">Mandatory field.<br/>For HDFS HA only.</dd><dd id="hdfs-name-service-example"><strong>Example:</strong><br/>lahdfs</dd><dd id="hdfs-name-service-other"><strong>Other notes:</strong><br/>It is available in <code>hdfs-site.xml</code> and defined as <code>dfs.nameservices</code>.</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-hdfs-name-node-ids"><dt>HDFS name node IDs</dt><dd id="hdfs-name-node-ids-description">Provides the list of NameNode IDs separted by comma and DataNodes use this property to determine all the NameNodes in the cluster.
XML property name is <code>dfs.ha.namenodes.<em>dfs.nameservices</em></code>.</dd><dd id="hdfs-name-node-ids-required">Mandatory field.<br/>For HDFS HA only.</dd><dd id="hdfs-name-node-ids-example"><strong>Example:</strong><br/>nn1,nn2</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-rpc-address-for-namenode1"><dt>RPC address for namenode1</dt><dd id="rpc-address-for-namenode1-description">To specify the fully-qualified RPC address for each listed NameNode and defined as <code>dfs.namenodes.rpc-address.<em>dfs.nameservices</em>.<em>name_node_ID_1></em></code>.</dd><dd id="rpc-address-for-namenode1-required">Mandatory field.<br/>For HDFS HA only.</dd><dd id="rpc-address-for-namenode1-example"><strong>Example:</strong><br/>www.example1.com:1234</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-rpc-address-for-namenode2"><dt>RPC address for namenode2</dt><dd id="rpc-address-for-namenode2-description">To specify the fully-qualified RPC address for each listed NameNode and defined as <code>dfs.namenode.rpc-address.<em>dfs.nameservices</em>.<em>name_node_ID_2</em></code>.</dd><dd id="rpc-address-for-namenode2-required">Mandatory field.<br/>For HDFS HA only.</dd><dd id="rpc-address-for-namenode2-example"><strong>Example:</strong><br/>www.example2.com:1234</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-dfs-host"><dt>DFS host</dt><dd id="dfs-host-description">Specify the DFS hostname or the IP address</dd><dd id="dfs-host-required">Mandatory field.<br/>For when <em>not</em> using HDFS HA.</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-dfs-port"><dt>DFS port</dt><dd id="dfs-port-description">Speciffy the associated DFS port</dd><dd id="dfs-port-required">Mandatory field.<br/>For when <em>not</em> using HDFS HA.</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-default-hdfs-location"><dt>Default HDFS location</dt><dd id="default-hdfs-location-description">Specify the location for the default source/target location</dd><dd id="default-hdfs-location-required">Mandatory field.</dd><dd id="default-hdfs-location-example"><strong>Example:</strong><br/>/tmp</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-temp-hdfs-location"><dt>Temp HDFS location</dt><dd id="temp-hdfs-location-description">Specify the location for creating temp directory</dd><dd id="temp-hdfs-location-required">Mandatory field.</dd><dd id="temp-hdfs-location-example"><strong>Example:</strong><br/>/tmp</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-hdfs-security-authentication"><dt>HDFS security authentication</dt><dd id="hdfs-security-authentication-description">Select the type of security being enabled </dd><dd id="hdfs-security-authentication-required">Mandatory field.</dd><dd id="hdfs-security-authentication-example"><strong>Example:</strong><br/>Kerberos</dd><dd id="hdfs-security-authentication-valid-values"><strong>Valid Values:</strong><br/>Simple, Kerberos</dd><dd id="hdfs-security-authentication-default"><strong>Default:</strong><br/>simple</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-hadoop-rpc-protection"><dt>Hadoop RPC protection</dt><dd id="hadoop-rpc-protection-description">Hadoop cluster administrators control the quality of protection using the configuration parameter hadoop.rpc.protection</dd><dd id="hadoop-rpc-protection-required">Mandatory field.<br/>For DFS security authentication with Kerberos only.</dd><dd id="hadoop-rpc-protection-example"><strong>Example:</strong><br/>none</dd><dd id="hadoop-rpc-protection-valid-values"><strong>Valid Values:</strong><br/>None, authentication, integrity, privacy</dd><dd id="hadoop-rpc-protection-default"><strong>Default:</strong><br/>authentication</dd><dd id="hadoop-rpc-protection-other"><strong>Other notes:</strong><br/>It is available in <code>core-site.xml</code>.</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-hive-principal"><dt>Hive principal</dt><dd id="hive-principal-description">Principal for authenticating hive services </dd><dd id="hive-principal-required">Mandatory field.</dd><dd id="hive-principal-example"><strong>Example:</strong><br/>hive/host@name.example.com</dd><dd id="hive-principal-other"><strong>Other notes:</strong><br/>It is available in <code>hive-site.xml</code>.</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-user-principal"><dt>User principal</dt><dd id="user-principal-description">To authenticate via a key-tab you must have supporting key-tab file which is generated by Kerberos Admin and also requires the user principal associated with Key-tab ( Configured while enabling Kerberos)</dd><dd id="user-principal-required">Mandatory field.</dd><dd id="user-principal-example"><strong>Example:</strong><br/>labuser@name.example.com</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-user-keytab"><dt>User keytab</dt><dd id="user-keytab-description">To authenticate via a key-tab you must have supporting key-tab file which is generated by Kerberos Admin and also requires the user principal associated with Key-tab ( Configured while enabling Kerberos)</dd><dd id="user-keytab-required">Mandatory field.</dd><dd id="user-keytab-example"><strong>Example:</strong><br/>/app/keytabs/labuser.keytab</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-kdc-host"><dt>KDC host</dt><dd id="kdc-host-description">Specify KDC Host Name where as KDC (Kerberos Key Distribution Center) is a service than runs on a domain controller server role (Configured from Kerbores configuration-/etc/krb5.conf )</dd><dd id="kdc-host-required">Mandatory field.</dd><dd id="kdc-host-example"><strong>Example:</strong><br/>kdc_host@example.com</dd></dlentry>
<dlentry id="dataflow-hdfs-conn-default-realm"><dt>Default realm</dt><dd id="default-realm-description">A Kerberos realm is the domain over which a Kerberos authentication server has the authority to authenticate a user, host or service (Configured from Kerbores configuration-/etc/krb5.conf )</dd><dd id="default-realm-required">Mandatory field.</dd><dd id="default-realm-example"><strong>Example:</strong><br/>name.example.com</dd></dlentry>
</dl>

## Sync properties

<dl id="dataflow-hdfs-sync-properties">
<dlentry id="dataflow-hdfs-sync-column-delimiter"><dt>Column delimiter</dt><dd id="column-delimiter-description">Specify the column delimiter character.</dd><dd id="column-delimiter-required">Mandatory field.</dd><dd id="column-delimiter-example"><strong>Example:</strong><br/>1</dd><dd id="column-delimiter-valid-values"><strong>Valid Values:</strong><br/>Any ASCII character</dd><dd id="column-delimiter-default"><strong>Default:</strong><br/>ASCII 01 (SOH)</dd></dlentry>
<dlentry id="dataflow-hdfs-sync-enable-archive-on-success"><dt>Enable archive on success</dt><dd id="enable-archive-on-success-description">Specify if data needs to be archived once it is succeeded</dd><dd id="enable-archive-on-success-required">Optional field.</dd><dd id="enable-archive-on-success-example"><strong>Example:</strong><br/>No</dd><dd id="enable-archive-on-success-valid-values"><strong>Valid Values:</strong><br/>Yes</dd><dd id="enable-archive-on-success-default"><strong>Default:</strong><br/>No</dd></dlentry>
<dlentry id="dataflow-hdfs-sync-delete-on-success"><dt>Delete on success</dt><dd id="delete-on-success-description">Specify if data needs to be deleted after execution is successful</dd><dd id="delete-on-success-required">Optional field.</dd><dd id="delete-on-success-example"><strong>Example:</strong><br/>No</dd><dd id="delete-on-success-valid-values"><strong>Valid Values:</strong><br/>Yes</dd><dd id="delete-on-success-default"><strong>Default:</strong><br/>No</dd></dlentry>
<dlentry id="dataflow-hdfs-sync-compression"><dt>Compression</dt><dd id="compression-description">Specify this if the file is compressed and what kind of compressed file it is</dd><dd id="compression-required">Mandatory field.</dd><dd id="compression-example"><strong>Example:</strong><br/>gzip</dd><dd id="compression-valid-values"><strong>Valid Values:</strong><br/>None, gzip</dd><dd id="compression-default"><strong>Default:</strong><br/>None</dd></dlentry>
<dlentry id="dataflow-hdfs-sync-enclosing-character"><dt>Enclosing character</dt><dd id="enclosing-character-description">Specify if the text columns in the source data needs to be enclosed in quotes.</dd><dd id="enclosing-character-required">Optional field.</dd><dd id="enclosing-character-example"><strong>Example:</strong><br/>Single</dd><dd id="enclosing-character-valid-values"><strong>Valid Values:</strong><br/>Single, Double, Empty</dd><dd id="enclosing-character-default"><strong>Default:</strong><br/>Double</dd></dlentry>
<dlentry id="dataflow-hdfs-sync-escape-character"><dt>Escape character</dt><dd id="escape-character-description">Specify the escape character if using a text qualifier in the source data.</dd><dd id="escape-character-required">Optional field.</dd><dd id="escape-character-example"><strong>Example:</strong><br/>\\</dd><dd id="escape-character-valid-values"><strong>Valid Values:</strong><br/>Any ASCII character</dd><dd id="escape-character-default"><strong>Default:</strong><br/>Empty</dd></dlentry>
<dlentry id="dataflow-hdfs-sync-null-value"><dt>Null value</dt><dd id="null-value-description">Specify the string literal that represents NULL values in data. During the data load, the column value that matches this string loads as NULL into ThoughtSpot.</dd><dd id="null-value-required">Optional field.</dd><dd id="null-value-example"><strong>Example:</strong><br/>NULL</dd><dd id="null-value-valid-values"><strong>Valid Values:</strong><br/>NULL</dd><dd id="null-value-default"><strong>Default:</strong><br/>NULL</dd></dlentry>
<dlentry id="dataflow-hdfs-sync-date-style"><dt>Date style</dt><dd id="date-style-description">Specifies how to interpret the date format</dd><dd id="date-style-required">Optional field.</dd><dd id="date-style-example"><strong>Example:</strong><br/>YMD</dd><dd id="date-style-valid-values"><strong>Valid Values:</strong><br/>YMD, MDY, DMY, DMONY, MONDY, Y2MD, MDY2, DMY2, DMONY2, MONDY2</dd><dd id="date-style-default"><strong>Default:</strong><br/>YMD</dd></dlentry>
<dlentry id="dataflow-hdfs-sync-date-delimiter"><dt>Date delimiter</dt><dd id="date-delimiter-description">Specifies the separator used in the date format ( only default delimiter is supported).</dd><dd id="date-delimiter-required">Optional field.</dd><dd id="date-delimiter-example"><strong>Example:</strong><br/>-</dd><dd id="date-delimiter-valid-values"><strong>Valid Values:</strong><br/>Any printable ASCII character</dd><dd id="date-delimiter-default"><strong>Default:</strong><br/>-</dd></dlentry>
<dlentry id="dataflow-hdfs-sync-time-style"><dt>Time style</dt><dd id="time-style-description">Specifies the format of the time portion in the data.</dd><dd id="time-style-required">Optional field.</dd><dd id="time-style-example"><strong>Example:</strong><br/>24HOUR</dd><dd id="time-style-valid-values"><strong>Valid Values:</strong><br/>12 HOUR</dd></dlentry>
<dlentry id="dataflow-hdfs-sync-time-delimiter"><dt>Time delimiter</dt><dd id="time-delimiter-description">Specifies the character used as separate the time components. (Only default delimiter is supported)</dd><dd id="time-delimiter-required">Optional field.</dd><dd id="time-delimiter-example"><strong>Example:</strong><br/>:</dd><dd id="time-delimiter-valid-values"><strong>Valid Values:</strong><br/>Any printable ASCII character</dd><dd id="time-delimiter-default"><strong>Default:</strong><br/>:</dd></dlentry>
<dlentry id="dataflow-hdfs-sync-ts-load-options"><dt>TS load options</dt><dd id="ts-load-options-description">Specify additional parameters passed with the <code>tsload</code> command. The format for these parameters is:<br/><code>--&lt;param_1_name&gt; &lt;optional_param_1_value&gt;</code></dd><dd id="ts-load-options-required">Optional field.</dd><dd id="ts-load-options-example"><strong>Example:</strong><br/><code>--max_ignored_rows 0</code></dd><dd id="ts-load-options-valid-values"><strong>Valid Values:</strong><br/><br/><code> --null_value ""</code><br/><code> --escape_character ""</code><br/><code> --max_ignored_rows 0</code></dd><dd id="ts-load-options-default"><strong>Default:</strong><br/><code> --max_ignored_rows 0</code></dd><dd id="reference"><strong>Reference:</strong><br/><a href="{{ site.baseurl }}/reference/data-importer-ref.html">tsload flag reference</a></dd></dlentry>
</dl>

## Related Information

[Dataflow tips]({{ site.baseurl }}/data-integrate/data-flow-tips.html)
