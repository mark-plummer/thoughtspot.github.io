---
title: [Hive connection reference]
summary: Learn about the fields used to create a Hive connection with ThoughtSpot DataFlow.
last_updated: 06/19/2020
redirect_from:
- /7.0.0.mar.sw/data-integrate/dataflow/dataflow-hive-reference.html
- /7.0.1.jun.sw/data-integrate/dataflow/dataflow-hive-reference.html
sidebar: mydoc_sidebar
permalink: /:collection/:path.html
---

Here is a list of the fields for a Hive connection in ThoughtSpot DataFlow. You need specific information to establish a seamless and secure connection.

## Connection properties

<dl id="dataflow-hive-connection-properties">
<dlentry id="dataflow-hive-conn-connection-name"><dt>Connection name</dt><dd id="connection-name-description">Name your connection.</dd><dd id="connection-name-required">Mandatory field.</dd><dd id="connection-name-example"><strong>Example:</strong><br/>HiveConnection</dd></dlentry>
<dlentry id="dataflow-hive-conn-connection-type"><dt>Connection type</dt><dd id="connection-type-description">Choose the Hive connection type.</dd><dd id="connection-type-required">Mandatory field.</dd><dd id="connection-type-example"><strong>Example:</strong><br/>Hive</dd></dlentry>
<dlentry id="dataflow-hive-conn-hiveserver2-ha-configured"><dt>HiveServer2 HA configured</dt><dd id="hiveserver2-ha-configured-description">Specify this option if using HiveServer2 High Availability.</dd><dd id="hiveserver2-ha-configured-required">Mandatory field.</dd></dlentry>
<dlentry id="dataflow-hive-conn-hiveserver2-zookeeper-namespace"><dt>HiveServer2 zookeeper namespace</dt><dd id="hiveserver2-zookeeper-namespace-description">Specify zookeeper namespace as hivesever2. This is the default value.</dd><dd id="hiveserver2-zookeeper-namespace-required">Mandatory field.<br/>Only when using Hiveserver2 HA.</dd><dd id="hiveserver2-zookeeper-namespace-example"><strong>Example:</strong><br/>hiveserver2</dd><dd id="hiveserver2-zookeeper-namespace-other"><strong>Other notes:</strong><br/>If value is different then, the value can be found from hive-site.xml against the property <code>hive.server2.zookeeper.namespace</code>.</dd></dlentry>
<dlentry id="dataflow-hive-conn-host"><dt>Host</dt><dd id="host-description">Specify the hostname or the IP address of the Hadoop system</dd><dd id="host-required">Mandatory field.<br/>Only when <em>not</em> using Hiveserver2 HA.</dd><dd id="host-example"><strong>Example:</strong><br/>myEmail@example.com</dd></dlentry>
<dlentry id="dataflow-hive-conn-port"><dt>Port</dt><dd id="port-description">Specify the port.</dd><dd id="port-required">Mandatory field.<br/>Only when <em>not</em> using Hiveserver2 HA.</dd><dd id="port-example"><strong>Example:</strong><br/>1234</dd></dlentry>
<dlentry id="dataflow-hive-conn-hive-security-authentication"><dt>Hive security authentication</dt><dd id="hive-security-authentication-description">Specifies the type of security protocol to connect to the instance. Based on the type of security select the authentication type and provide details.</dd><dd id="hive-security-authentication-required">Mandatory field.</dd><dd id="hive-security-authentication-example"><strong>Example:</strong><br/>Kerberos</dd><dd id="hive-security-authentication-valid-values"><strong>Valid Values:</strong><br/>Simple, Kerberos, LDAP, SSL, Kerberos & SSL, LDAP & SSL</dd><dd id="hive-security-authentication-default"><strong>Default:</strong><br/>Simple</dd><dd id="hive-security-authentication-other"><strong>Other notes:</strong><br/>The authentication type setup for the instance can be found from hive-site.xml against the property <code>hive.server2.authentication</code>.</dd></dlentry>
<dlentry id="dataflow-hive-conn-user"><dt>User</dt><dd id="user-description">Specify the user to connect to Hive. This user must have data access privileges.</dd><dd id="user-required">Mandatory field.<br/>For simple, LDAP, and Simple authentication only.</dd><dd id="user-example"><strong>Example:</strong><br/>userdi</dd><dd id="user-default"><strong>Default:</strong><br/>simple</dd></dlentry>
<dlentry id="dataflow-hive-conn-password"><dt>Password</dt><dd id="password-description">Specify the password.</dd><dd id="password-required">Optional field.<br/>For simple, LDAP authentication only.</dd><dd id="password-example"><strong>Example:</strong><br/>pswrd234%!</dd></dlentry>
<dlentry id="dataflow-hive-conn-trust-store"><dt>Trust store</dt><dd id="trust-store-description">Specify the trust store name for authentication</dd><dd id="trust-store-required">Mandatory field.<br/>For SSL and Kerberos & SSL authentication only.</dd><dd id="trust-store-example"><strong>Example:</strong><br/>trust store</dd><dd id="trust-store-default"><strong>Default:</strong><br/>SSL</dd></dlentry>
<dlentry id="dataflow-hive-conn-trust-store-password"><dt>Trust store password</dt><dd id="trust-store-password-description">Specify the password for the trust store</dd><dd id="trust-store-password-required">Mandatory field.<br/>For SSL and Kerberos & SSL authentication only.</dd><dd id="trust-store-password-example"><strong>Example:</strong><br/>password</dd><dd id="trust-store-password-default"><strong>Default:</strong><br/>SSL</dd></dlentry>
<dlentry id="dataflow-hive-conn-hive-transport-mode"><dt>Hive transport mode</dt><dd id="hive-transport-mode-description">Applicable only for hive process engine. This specifies the network protocol used for communicating between hive nodes. </dd><dd id="hive-transport-mode-required">Mandatory field.</dd><dd id="hive-transport-mode-example"><strong>Example:</strong><br/>binary</dd><dd id="hive-transport-mode-valid-values"><strong>Valid Values:</strong><br/>Binary, HTTP</dd><dd id="hive-transport-mode-default"><strong>Default:</strong><br/>binary</dd><dd id="hive-transport-mode-other"><strong>Other notes:</strong><br/>The Hive transport mode can be identified from hive-site.xml against the property hive.server2.transport.mode.</dd></dlentry>
<dlentry id="dataflow-hive-conn-http-path"><dt>HTTP path</dt><dd id="http-path-description">This is specified as an option when http transport mode is selected</dd><dd id="http-path-required">Mandatory field.<br/>For HTTP transport mode only.</dd><dd id="http-path-example"><strong>Example:</strong><br/>cliservice</dd><dd id="http-path-valid-values"><strong>Valid Values:</strong><br/>cliservice</dd><dd id="http-path-default"><strong>Default:</strong><br/>cliservice</dd><dd id="http-path-other"><strong>Other notes:</strong><br/>The HTTP Path value can be identified from <code>hive-site.xml</code> against the property <code>hive.server2.thrift.http.path</code>.</dd></dlentry>
<dlentry id="dataflow-hive-conn-hadoop-distribution"><dt>Hadoop distribution</dt><dd id="hadoop-distribution-description">Provide the distribution of Hadoop being connected to</dd><dd id="hadoop-distribution-required">Mandatory field.</dd><dd id="hadoop-distribution-example"><strong>Example:</strong><br/>Hortonworks</dd><dd id="hadoop-distribution-valid-values"><strong>Valid Values:</strong><br/>CDH, Hortonworks, EMR</dd><dd id="hadoop-distribution-default"><strong>Default:</strong><br/>CDH</dd></dlentry>
<dlentry id="dataflow-hive-conn-distribution-version"><dt>Distribution version</dt><dd id="distribution-version-description">Provide the version of the Distribution chosen above</dd><dd id="distribution-version-required">Mandatory field.</dd><dd id="distribution-version-example"><strong>Example:</strong><br/>2.6.5</dd><dd id="distribution-version-valid-values"><strong>Valid Values:</strong><br/>Any Numeric value</dd><dd id="distribution-version-default"><strong>Default:</strong><br/>6.3.x</dd></dlentry>
<dlentry id="dataflow-hive-conn-hadoop-conf-path"><dt>Hadoop conf path</dt><dd id="hadoop-conf-path-description">By default, the system picks the Hadoop configuration files from the HDFS. To override, specify an alternate location. Applies only when using configuration settings that are different from global Hadoop instance settings.</dd><dd id="hadoop-conf-path-required">Mandatory field.</dd><dd id="hadoop-conf-path-example"><strong>Example:</strong><br/>$DI_HOME/app/path</dd><dd id="hadoop-conf-path-other"><strong>Other notes:</strong><br/>An instance where this could be needed is, if the hdfs is encrypted and the location of key files and password decrypt the files is available in the hadoop config files.</dd></dlentry>
<dlentry id="dataflow-hive-conn-dfs-ha-configured"><dt>DFS HA configured</dt><dd id="dfs-ha-configured-description">Specify if using High Availability for DFS.</dd><dd id="dfs-ha-configured-required">Optional field.<br/>For Hadoop Extract only.</dd><dd id="dfs-ha-configured-example"><strong>Example:</strong><br/>Checked</dd></dlentry>
<dlentry id="dataflow-hive-conn-dfs-name-service"><dt>DFS name service</dt><dd id="dfs-name-service-description">Specify the logical name of the HDFS nameservice. </dd><dd id="dfs-name-service-required">Mandatory field.<br/>For DFS HA and Hadoop Extract only.</dd><dd id="dfs-name-service-example"><strong>Example:</strong><br/>lahdfs</dd><dd id="dfs-name-service-other"><strong>Other notes:</strong><br/>It is available in hdfs-site.xml and defined as dfs.nameservices</dd></dlentry>
<dlentry id="dataflow-hive-conn-dfs-name-node-ids"><dt>DFS name node IDs</dt><dd id="dfs-name-node-ids-description">Specify a comma-separated list of NameNode IDs. System uses this property to determine all NameNodes in the cluster. XML property name is <code>dfs.ha.namenodes.<em>dfs.nameservices</em></code>.</dd><dd id="dfs-name-node-ids-required">Mandatory field.<br/>For DFS HA and Hadoop Extract only.</dd><dd id="dfs-name-node-ids-example"><strong>Example:</strong><br/>nn1, nn2</dd></dlentry>
<dlentry id="dataflow-hive-conn-rpc-address-for-namenode1"><dt>RPC address for namenode1</dt><dd id="rpc-address-for-namenode1-description">Specify the fully-qualified RPC address for each listed NameNode. Defined as <code>dfs.namenode.rpc-address.<em>dfs.nameservices</em>.<em>name node ID 1</em></code>.</dd><dd id="rpc-address-for-namenode1-required">Mandatory field.<br/>For DFS HA and Hadoop Extract only.</dd><dd id="rpc-address-for-namenode1-example"><strong>Example:</strong><br/>lclabh.example.com:5678</dd></dlentry>
<dlentry id="dataflow-hive-conn-rpc-address-for-namenode2"><dt>RPC address for namenode2</dt><dd id="rpc-address-for-namenode2-description">Specify the fully-qualified RPC address for each listed NameNode. Define as <code>dfs.namenode.rpc-address.<em>dfs.nameservices</em>.<em>name node ID 2</em></code>.</dd><dd id="rpc-address-for-namenode2-required">Mandatory field.<br/>For DFS HA and Hadoop Extract only.</dd><dd id="rpc-address-for-namenode2-example"><strong>Example:</strong><br/>lvclabh.example.com:9876</dd></dlentry>
<dlentry id="dataflow-hive-conn-dfs-host"><dt>DFS host</dt><dd id="dfs-host-description">Specify the DFS hostname or the IP address</dd><dd id="dfs-host-required">Mandatory field.<br/>For Hadoop Extract only, when <em>not</em> using DFS HA.</dd><dd id="dfs-host-example"><strong>Example:</strong><br/>myemail@example.com</dd></dlentry>
<dlentry id="dataflow-hive-conn-dfs-port"><dt>DFS port</dt><dd id="dfs-port-description">Specify the associated DFS port</dd><dd id="dfs-port-required">Mandatory field.<br/>For Hadoop Extract only, when <em>not</em> using DFS HA.</dd><dd id="dfs-port-example"><strong>Example:</strong><br/>1234</dd></dlentry>
<dlentry id="dataflow-hive-conn-default-dfs-location"><dt>Default DFS location</dt><dd id="default-dfs-location-description">Specify the location for the default source/target location</dd><dd id="default-dfs-location-required">Mandatory field.<br/>For Hadoop Extract only.</dd><dd id="default-dfs-location-example"><strong>Example:</strong><br/>/tmp</dd></dlentry>
<dlentry id="dataflow-hive-conn-temp-dfs-location"><dt>Temp DFS location</dt><dd id="temp-dfs-location-description">Specify the location for creating temp directory</dd><dd id="temp-dfs-location-required">Mandatory field.<br/>For Hadoop Extract only.</dd><dd id="temp-dfs-location-example"><strong>Example:</strong><br/>/tmp</dd></dlentry>
<dlentry id="dataflow-hive-conn-dfs-security-authentication"><dt>DFS security authentication</dt><dd id="dfs-security-authentication-description">Select the type of security being enabled </dd><dd id="dfs-security-authentication-required">Mandatory field.<br/>For Hadoop Extract only.</dd><dd id="dfs-security-authentication-example"><strong>Example:</strong><br/>Kerberos</dd><dd id="dfs-security-authentication-valid-values"><strong>Valid Values:</strong><br/>Simple, Kerberos</dd><dd id="dfs-security-authentication-default"><strong>Default:</strong><br/>simple</dd></dlentry>
<dlentry id="dataflow-hive-conn-hadoop-rpc-protection"><dt>Hadoop RPC protection</dt><dd id="hadoop-rpc-protection-description">Hadoop cluster administrators control the quality of protection using the configuration parameter <code>hadoop.rpc.protection</code>.</dd><dd id="hadoop-rpc-protection-required">Mandatory field.<br/>When using Kerberos DFS security authentication <em>and</em> Hadoop Extract.</dd><dd id="hadoop-rpc-protection-example"><strong>Example:</strong><br/>none</dd><dd id="hadoop-rpc-protection-valid-values"><strong>Valid Values:</strong><br/>None, authentication, integrity, privacy</dd><dd id="hadoop-rpc-protection-default"><strong>Default:</strong><br/>authentication</dd><dd id="hadoop-rpc-protection-other"><strong>Other notes:</strong><br/>It is available in core-site.xml.</dd></dlentry>
<dlentry id="dataflow-hive-conn-hive-principal"><dt>Hive principal</dt><dd id="hive-principal-description">Principal for authenticating hive services </dd><dd id="hive-principal-required">Mandatory field.</dd><dd id="hive-principal-example"><strong>Example:</strong><br/>hive/host@lab.example.com</dd><dd id="hive-principal-other"><strong>Other notes:</strong><br/>It is available in hive-site.xml</dd></dlentry>
<dlentry id="dataflow-hive-conn-user-principal"><dt>User principal</dt><dd id="user-principal-description">To authenticate via a key-tab you must have supporting key-tab file which is generated by Kerberos Admin and also requires the user principal associated with Key-tab ( Configured while enabling Kerberos)</dd><dd id="user-principal-required">Mandatory field.</dd><dd id="user-principal-example"><strong>Example:</strong><br/>labuser@labdp.example.com</dd></dlentry>
<dlentry id="dataflow-hive-conn-user-keytab"><dt>User keytab</dt><dd id="user-keytab-description">To authenticate via a key-tab you must have supporting key-tab file which is generated by Kerberos Admin and also requires the user principal associated with Key-tab ( Configured while enabling Kerberos)</dd><dd id="user-keytab-required">Mandatory field.</dd><dd id="user-keytab-example"><strong>Example:</strong><br/>/app/keytabs/labuser.keytab</dd></dlentry>
<dlentry id="dataflow-hive-conn-kdc-host"><dt>KDC host</dt><dd id="kdc-host-description">Specify KDC Host Name where as KDC (Kerberos Key Distribution Center) is a service than runs on a domain controller server role (Configured from Kerbores configuration-/etc/krb5.conf )</dd><dd id="kdc-host-required">Mandatory field.</dd><dd id="kdc-host-example"><strong>Example:</strong><br/>example.example.com</dd></dlentry>
<dlentry id="dataflow-hive-conn-default-realm"><dt>Default realm</dt><dd id="default-realm-description">A Kerberos realm is the domain over which a Kerberos authentication server has the authority to authenticate a user, host or service (Configured from Kerbores configuration-/etc/krb5.conf )</dd><dd id="default-realm-required">Mandatory field.</dd><dd id="default-realm-example"><strong>Example:</strong><br/>labhdp.example.com</dd></dlentry>
<dlentry id="dataflow-hive-conn-queue-name"><dt>Queue name</dt><dd id="queue-name-description">Specify the queue name followed by a coma separated form in yarn.scheduler.capacity.root.queues. </dd><dd id="queue-name-required">Mandatory field.<br/>For Hadoop Extract only.</dd><dd id="queue-name-example"><strong>Example:</strong><br/>default</dd><dd id="queue-name-other"><strong>Other notes:</strong><br/>It is available in capacity-scheduler.xml</dd></dlentry>
<dlentry id="dataflow-hive-conn-yarn-web-ui-port"><dt>YARN web UI port</dt><dd id="yarn-web-ui-port-description">Yarn Providing web UI for yarn RM and by default 8088 in use</dd><dd id="yarn-web-ui-port-required">Mandatory field.<br/>For Hadoop Extract only.</dd><dd id="yarn-web-ui-port-example"><strong>Example:</strong><br/>8088</dd></dlentry>
<dlentry id="dataflow-hive-conn-zookeeper-quorum-host"><dt>Zookeeper quorum host</dt><dd id="zookeeper-quorum-host-description">Specify the value of hadoop.registry.zk.quorum from yarn-site.xml</dd><dd id="zookeeper-quorum-host-required">Mandatory field.<br/>Only when <em>not</em> using Hiveserver2 HA.</dd><dd id="zookeeper-quorum-host-example"><strong>Example:</strong><br/>lvclhdp1.example.com:21,lvclabhdp12.example.com:81,lvclabhdp12.example.com:2093</dd></dlentry>
<dlentry id="dataflow-hive-conn-yarn-timeline-webapp-host"><dt>Yarn timeline webapp host</dt><dd id="yarn-timeline-webapp-host-description">Specify the ip adress of yarn timeline service web application </dd><dd id="yarn-timeline-webapp-host-required">Mandatory field.</dd><dd id="yarn-timeline-webapp-host-example"><strong>Example:</strong><br/>8188</dd></dlentry>
<dlentry id="dataflow-hive-conn-yarn-timeline-webapp-port"><dt>Yarn timeline webapp port</dt><dd id="yarn-timeline-webapp-port-description">Specify the port associated with the yarn timeline service web application </dd><dd id="yarn-timeline-webapp-port-required">Mandatory field.</dd><dd id="yarn-timeline-webapp-port-example"><strong>Example:</strong><br/>8190</dd></dlentry>
<dlentry id="dataflow-hive-conn-yarn-timeline-webapp-version"><dt>Yarn timeline webapp version</dt><dd id="yarn-timeline-webapp-version-description">Specify the version associated with the yarn timeline service web application </dd><dd id="yarn-timeline-webapp-version-required">Mandatory field.</dd><dd id="yarn-timeline-webapp-version-example"><strong>Example:</strong><br/>v1</dd></dlentry>
<dlentry id="dataflow-hive-conn-jdbc-options"><dt>JDBC options</dt><dd id="jdbc-options-description">Specify the options associated with the JDBC URL.</dd><dd id="jdbc-options-required">Optional field.</dd><dd id="jdbc-options-example"><strong>Example:</strong><br/><code>jdbc:sqlserver://[serverName[\instanceName][:portNumber]]</code></dd></dlentry>
</dl>

## Sync properties

<dl id="dataflow-hive-sync-properties">
<dlentry id="dataflow-hive-sync-data-extraction-mode"><dt>Data extraction mode</dt><dd id="data-extraction-mode-description">Specify the extraction type.</dd><dd id="data-extraction-mode-required">Mandatory field.</dd><dd id="data-extraction-mode-example"><strong>Example:</strong><br/>Hadoop Extract</dd><dd id="data-extraction-mode-valid-values"><strong>Valid Values:</strong><br/>Hadoop Extract, JDBC</dd><dd id="data-extraction-mode-default"><strong>Default:</strong><br/>Hadoop Extract</dd></dlentry>
<dlentry id="dataflow-hive-sync-null-value"><dt>Null value</dt><dd id="null-value-description">Specifies the string literal that should indicate the null value in the extracted data. During the data load the column value matching this string will be loaded as null in the target.</dd><dd id="null-value-required">Mandatory field.<br/>For Hadoop Extract only.</dd><dd id="null-value-example"><strong>Example:</strong><br/>NULL</dd><dd id="null-value-valid-values"><strong>Valid Values:</strong><br/>NULL</dd><dd id="null-value-default"><strong>Default:</strong><br/>NULL</dd></dlentry>
<dlentry id="dataflow-hive-sync-enclosing-character"><dt>Enclosing character</dt><dd id="enclosing-character-description">Specify if the text columns in the source data needs to be enclosed in quotes.</dd><dd id="enclosing-character-required">Mandatory field.</dd><dd id="enclosing-character-example"><strong>Example:</strong><br/>DOUBLE</dd><dd id="enclosing-character-valid-values"><strong>Valid Values:</strong><br/>SINGLE, DOUBLE</dd><dd id="enclosing-character-default"><strong>Default:</strong><br/>DOUBLE</dd></dlentry>
<dlentry id="dataflow-hive-sync-escape-character"><dt>Escape character</dt><dd id="escape-character-description">Specify the escape character if using a text qualifier in the source data.</dd><dd id="escape-character-required">Mandatory field.</dd><dd id="escape-character-example"><strong>Example:</strong><br/>\"</dd><dd id="escape-character-valid-values"><strong>Valid Values:</strong><br/>\\, Any ASCII character</dd><dd id="escape-character-default"><strong>Default:</strong><br/>\"</dd></dlentry>
<dlentry id="dataflow-hive-sync-ts-load-options"><dt>TS load options</dt><dd id="ts-load-options-description">Specifies the parameters passed with the <code>tsload</code> command, in addition to the commands already included by the application. The format for these parameters is:<br/><code> --&lt;param_1_name&gt; &lt;optional_param_1_value&gt;</code><br/><code> --&lt;param_2_name&gt; &lt;optional_param_2_value&gt;</code></dd><dd id="ts-load-options-required">Optional field.</dd><dd id="ts-load-options-example"><strong>Example:</strong><br/>--max_ignored_rows 0</dd><dd id="ts-load-options-valid-values"><strong>Valid Values:</strong><br/><br/><code> --null_value "</code><br/><code> --escape_character "</code><br/><code> --max_ignored_rows 0</code></dd><dd id="ts-load-options-default"><strong>Default:</strong><br/>--max_ignored_rows 0</dd><dd id="reference"><strong>Reference:</strong><br/><a href="{{ site.baseurl }}/reference/data-importer-ref.html">tsload flag reference</a></dd></dlentry></dl>

## Related Information

[Dataflow tips]({{ site.baseurl }}/data-integrate/data-flow-tips.html)
